import torch
import torch.nn as nn

import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.optim as optim

import imageio
import numpy as np

from torchvision.utils import  make_grid, save_image
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
from tqdm import tqdm     # progress bar

# learning parameters
# batch_size=512
batch_size=128
epochs=200
sample_size=64
nz=128
k=1
device =torch.device("cuda" if torch.cuda.is_available() else "cpu")

# prepare the dataset
transform=transforms.Compose([ transforms.ToTensor(),
                                transforms.Normalize((0.5,),(0.5,))

])

to_pil_image=transforms.ToPILImage()
train_data=datasets.MNIST(root=r"./data",
                          train=True,
                          download=True,
                          transform=transform)

train_loader =DataLoader(train_data, batch_size=batch_size, shuffle=True)

# the generator neural network
class Generator(nn.Module):
    def __init__(self, nz):
        super(Generator, self).__init__()
        self.nz =nz
        self.main = nn.Sequential(
            nn.Linear(self.nz, 256),
            nn.LeakyReLU(0.2),

            nn.Linear(256,512),
            nn.LeakyReLU(0.2),

            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),

            nn.Linear(1024, 784),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.main(x).view(-1, 1, 28, 28)

# the discriminator neural network


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator,self).__init__()
        self.input = 784
        self.main = nn.Sequential(
            nn.Linear(self.input, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),

            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            nn.LeakyReLU(0.2), # activation function, like hinge function
            nn.Dropout(0.3),

            nn.Linear(256, 1),
            nn.Sigmoid()         # activation function
        )

    def forward(self, x):
        x = x.view(-1, 784)
        return self.main(x)
generator=Generator(nz).to(device)
discriminator=Discriminator().to(device)

print("########### Generator ##########")
print(generator)
print("########### Discriminator ##########")
print(discriminator)

# optimizer
optimizer_g = optim.Adam(generator.parameters(), lr=0.002)
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.002)

# loss function
criterion = nn.BCELoss()  # binary cross entropy loss

losses_g = []    # list, to store the generator loss after each epoch
losses_d = []    # list, to store the discriminator loss after each epoch
images = []     # to store images generated by the generator

# define some utility  functions
def label_real(size):
    data = torch.ones(size, 1)
    return data.to(device)


def label_fake(size):
    data = torch.zeros(size, 1)
    return data.to(device)


def create_noise(sample_size, nz):
    return torch.randn(sample_size,nz).to(device)


def save_generator_image(image, path):
    save_image(image, path)

# train the discriminator network
def train_discriminator(optimizer, data_real, data_fake):
    b_size=data_real.size(0)
    real_label = label_real(b_size)
    fake_label = label_fake(b_size)

    optimizer.zero_grad()

    output_real = discriminator(data_real)
    loss_real = criterion(output_real,real_label)

    output_fake = discriminator(data_fake)
    loss_fake = criterion(output_fake, fake_label)

    loss_real.backward()    # compute the gradient
    loss_fake.backward()
    optimizer.step()        # update the parameters

    return loss_real+loss_fake

# train the generator network
def train_generator(optimizer, data_fake):
    b_size=data_fake.size(0)
    real_label=label_real(b_size)

    optimizer.zero_grad()

    output=discriminator(data_fake)
    loss = criterion(output,real_label)

    loss.backward()

    optimizer.step()

    return loss

# train the GAN
#create the noise vector
noise = create_noise(sample_size, nz)

generator.train()
discriminator.train()
#train the loop

for epoch in range(epochs):      #epochs stand the train loop
    loss_g = 0.0
    loss_d = 0.0
    for bi, data in tqdm(enumerate(train_loader),total=int(len(train_data)/train_loader.batch_size)):     # iterate through the batches
        image,_=data
        image =image.to(device)
        b_size =len(image)

        for step in range(k):
            data_fake = generator(create_noise(b_size, nz)).detach()  # require_gradient=false
            data_real =image

            loss_d+=train_discriminator(optimizer_d,data_real,data_fake)
        data_fake = generator(create_noise(b_size, nz))
        loss_g += train_generator(optimizer_g, data_fake)

    # create the final fake image for the epoch
    generated_img =generator(noise).cpu().detach()

    generated_img = make_grid(generated_img)
    save_generator_image(generated_img, f"gen_img{epoch}.png")

    images.append(generated_img)
    epoch_loss_g= loss_g/bi
    epoch_loss_d= loss_d/bi
    losses_g.append(epoch_loss_g)
    losses_d.append(epoch_loss_d)

    print(f"Epoch {epoch} of {epochs}")
    print(f"Generator loss :{epoch_loss_g:.8f}, Discriminator loss:{epoch_loss_d:.8f}")

# store the checkpiont and plot

print("done training")
torch.save(generator.state_dict(), f"../outputs/generator.pth")

#save the generated images as GIF file
imgs =[np.arrary(to_pil_image(img) for img in images)]
imageio.mimsave(f"../outputs/generator_images.gif", imgs)

#plot and save the generator loss and discriminator loss
plt.figure()
plt.plot(losses_g, label='G_loss')
plt.plot(losses_d, label='D_loss')
plt.legend()
plt.savefig()






